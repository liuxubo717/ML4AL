<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multimodal Learning for Audio and Language | ICASSP Special Session</title>
  <!-- Add your custom CSS styles -->
  <style>
    body {
      font-family: "Times New Roman", Times, serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      color: #333;
    }

    .container-lg {
      max-width: 900px; /* Adjust the width as desired */
      margin: 0 auto;
      padding: 20px;
    }

    header {
      text-align: center;
      margin-bottom: 20px;
    }

    header img {
      max-width: 100%;
    }

    h1 {
      font-size: 2.5rem;
      margin-bottom: 10px;
    }

    h2 {
      font-size: 1.8rem;
      margin-bottom: 10px;
    }

    h3 {
      font-size: 1.5rem;
      margin-bottom: 10px;
    }

    p {
      margin-bottom: 15px;
      text-align: justify; /* Add the justified alignment */
      font-size: 1.1rem; /* Slightly enlarge the font size */
    }

    ul {
      margin-left: 20px;
      margin-bottom: 15px;
      font-size: 1.1rem; /* Slightly enlarge the font size */
    }

    li {
      margin-bottom: 5px;
    }

    a {
      color: #007BFF;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }
  </style>

<!-- Search Engine Optimization (SEO) and social media sharing metadata -->
<meta name="description" content="Multimodal Learning for Audio and Language – ICASSP Special Session">
<meta property="og:title" content="Multimodal Learning for Audio and Language">
<meta property="og:locale" content="en_US">
<meta property="og:description" content="Multimodal Learning for Audio and Language – ICASSP Special Session">
<meta property="og:image" content="https://github.com/liuxubo717/ML4AL/blob/main/icassp.png">
<meta property="og:url" content="https://example.com/multimodal-learning">
<meta property="og:site_name" content="Multimodal Learning for Audio and Language">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Multimodal Learning for Audio and Language">
<meta property="twitter:description" content="Multimodal Learning for Audio and Language – ICASSP Special Session">
<meta property="twitter:image" content="https://github.com/liuxubo717/ML4AL/blob/main/icassp.png">
</head>
<body>
    <header>
        <img src="icassp.png" alt="ICASSP Logo">
    </header>
  <div class="container-lg">
    <h1>Multimodal Learning for Audio and Language</h1>
    <h2>Special Session at <a href="https://2024.ieeeicassp.org/">ICASSP 2024</a></h2>
    <p>Sounds carry a wide range of information about environments, from individual physical events to sound scenes. Using deep learning and machine learning methods for analysis, recognition and synthesis of sounds has achieved remarkable results in recent years. Human beings perceive the world via multimodal information, we hear the sounds and communicate in language. Recently, the field of audio and language has emerged as an important research area in audio signal processing and natural language processing. Multi-modal audio-language tasks hold immense potential in various application scenarios. For instance, automatic audio captioning aims to provide meaningful language descriptions of audio content, benefiting the hearing-impaired in comprehending environmental sounds. Language-based audio retrieval facilitates efficient multimedia content retrieval and sound analysis for security surveillance. Text-queried audio source separation aims to separate  an arbitrary sound from an audio mixture, which offers a flexible user-interface in future audio editing and creation applications. Text-to-audio generation endeavors to synthesize audio content based on language descriptions, serving as sound synthesis tools for film making, game design, virtual reality, digital media, and aiding text understanding for the visually impaired.  However, audio-language multimodal learning presents challenges in comprehending the audio events and scenes within an audio clip, as well as interpreting the textual information presented in natural language. Furthermore, the limited size of existing audio-language datasets hampers generalization in real-world scenarios. This special session aims to present a collection of recent advances in the area of audio-language multimodal learning, from leading researchers, and exchange the ideas from researchers and identify the potential new research directions for further advancement of the field.</p>

    <h3>Potential topics of interest include but are not limited to:</h3>
    <ul>
      <li>Audio large language models (LLMs)</li>
      <li>Automatic audio captioning</li>
      <li>Language-based audio retrieval</li>
      <li>Audio question answering</li>
      <li>Text-to-audio generation</li>
      <li>Text-queried audio source separation</li>
      <li>Audio-language representation learning</li>
      <li>New datasets and tasks for audio-language learning</li>
      <li>New performance metrics for evaluation of audio-language tasks</li>
    </ul>

    <h3 id="paper-submission">Paper submission</h3>
    <p>It is important that the special session authors not submit their papers through the regular ICASSP 2024 paper submission process, but rather they use the following URL and select our special session (20.14: Multimodal Learning for Audio and Language) from the "Special Session Name" menu. Papers submitted to the session should follow the regular ICASSP paper guidelines and review process. Accepted papers will appear in the main proceedings and the IEEE Xplore.</p>
    
    <p>Papers must be submitted by 6 September 2023.</p>

    <h3 id="organising-team">Organising team</h3>
    <p>Xubo Liu, University of Surrey, UK</p>
    <p>Mark D. Plumbley, University of Surrey, UK</p>
    <p>Wenwu Wang, University of Surrey, UK</p>
  </div>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js
